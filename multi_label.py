# -*- coding: utf-8 -*-
"""multi-label.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G6KD8jOfcnSS2S_Cktau2W1GexzUIMho
"""

# !wget http://nlp.stanford.edu/data/glove.6B.zip
# !unzip glove.6B.zip

# #The dataset we use can be downloaded from Kaggle(https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview)
# !unzip train.csv.zip
import pandas as pd
df = pd.read_csv("train.csv")

import pandas as pd
# !unzip test.csv.zip
df_test = pd.read_csv("test.csv")

# !unzip test.csv.zip
# !unzip test_labels.csv.zip

labels = df[["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]]
label_counts = labels.sum(axis=0)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib
# matplotlib.use('Agg')
# matplotlib.use('TkAgg')
# %matplotlib inline

import matplotlib.pyplot as plt
plt.figure(figsize=(10,8))
barlist = plt.bar(labels.columns, label_counts)
plt.xticks(labels.columns, label_counts, rotation=60 , size=15)
plt.ylabel('Counts',size=20)
plt.xlabel('Labels',size=20)

plt.show()

df_test_label = pd.read_csv("test_labels.csv")

import re
def preprocess_text(sen):
    sentence = re.sub('[^a-zA-Z]', ' ', sen)
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)
    sentence = re.sub(r'\s+', ' ', sentence)
    return sentence

fig , axes = plt.subplots(2,3,figsize = (10,10), constrained_layout = True)
sns.countplot(ax=axes[0,0],x='toxic',data = df_test_label )
sns.countplot(ax=axes[0,1],x='severe_toxic',data = df_test_label)
sns.countplot(ax=axes[0,2],x='obscene',data = df_test_label)
sns.countplot(ax = axes[1,0],x='threat',data = df_test_label)
sns.countplot(ax=axes[1,1],x='insult',data = df_test_label)
sns.countplot(ax=axes[1,2],x='identity_hate',data = df_test_label)
plt.suptitle('No Of Classes Of Each Category')
plt.show()

df_test = df_test[(df_test_label.severe_toxic > -1) & 
              (df_test_label.toxic > -1) & 
              (df_test_label.obscene > -1) & 
              (df_test_label.threat > -1) & 
              (df_test_label.insult > -1) & 
              (df_test_label.identity_hate > -1) ]

df_test_label = df_test_label[(df_test_label.severe_toxic > -1) & 
              (df_test_label.toxic > -1) & 
              (df_test_label.obscene > -1) & 
              (df_test_label.threat > -1) & 
              (df_test_label.insult > -1) & 
              (df_test_label.identity_hate > -1) ]

df_test_label = df_test_label.drop(['id'], axis=1)
df_test_label = df_test_label.reset_index()
df_test = df_test.reset_index()

df_test_label = df_test_label.drop(['index'], axis=1)

df['clean'] = df["comment_text"].apply(preprocess_text)
df_test['clean'] = df_test["comment_text"].apply(preprocess_text)

from sklearn.model_selection import train_test_split

def split_train_test(X_train, Y_train, X_test, Y_test, test_size = 0.2, shuffle_state = True ):
    FEATURES = ['clean']
    X_train, X_test, Y_train, Y_test = [X_train[FEATURES],
                                       X_test[FEATURES],
                                       Y_train,
                                       Y_test]
    # X_train, X_test, Y_train, Y_test = train_test_split(
    #                                                     X[FEATURES],
    #                                                     Y,
    #                                                     shuffle = shuffle_state,
    #                                                     test_size = test_size, 
    #                                                     random_state = 32)
    # X_train = X_train.reset_index()
    # X_test = X_test.reset_index()

    return X_train, X_test, Y_train, Y_test 
X_train, X_test, Y_train, Y_test = split_train_test(df, labels.values, df_test, df_test_label.values )

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
plt.style.use('classic')
# %matplotlib inline
import seaborn as sns
from scipy import stats
#Observe distribution

X_train['freq_word'] = X_train['clean'].apply(lambda x: len(str(x).split()))
X_train['unique_freq_word'] = X_train['clean'].apply(lambda x: len(set(str(x).split())))
                                                 
X_test['freq_word'] = X_test['clean'].apply(lambda x: len(str(x).split()))
X_test['unique_freq_word'] = X_test['clean'].apply(lambda x: len(set(str(x).split())))                  

fig, axes = plt.subplots(ncols=2)
fig.set_size_inches(10,5)

sns.distplot(X_train['freq_word'], bins = 90, ax=axes[0], fit = stats.norm)
(mu0, sigma0) = stats.norm.fit(X_train['freq_word'])
axes[0].legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu0, sigma0)],loc='best')
axes[0].set_title("Distribution Word Frequency")
axes[0].axvline(X_train['freq_word'].median(), linestyle='dashed')
print("median of word frequency: ", X_train['freq_word'].median())


sns.distplot(X_train['unique_freq_word'], bins = 90, ax=axes[1], color = 'r', fit = stats.norm)
(mu1, sigma1) = stats.norm.fit(X_train['unique_freq_word'])
axes[1].set_title("Distribution Unique Word Frequency")
axes[1].legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu1, sigma1)],loc='best')
axes[1].axvline(X_train['unique_freq_word'].median(), linestyle='dashed')
print("median of uniuqe word frequency: ", X_train['unique_freq_word'].median())

# Commented out IPython magic to ensure Python compatibility.
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# %matplotlib inline
def cloud(data,backgroundcolor = 'white', width = 800, height = 600):
    wordcloud = WordCloud(stopwords = STOPWORDS, background_color = backgroundcolor,
                         width = width, height = height).generate(data)
    plt.figure(figsize = (15, 10))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()
    
cloud(' '.join(X_train['clean']))

MAX_FEATURES = 5000
EMBEDDING_DIM = 200
VALIDATION_SPLIT = 0.2
MAXLEN =  255 #base on the standard diviaion and mean, we could set this to 255 and cover 95% sentence
BATCH_SIZE = 32
nb_classes =2

from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.utils import np_utils

tokenizer = Tokenizer(num_words = MAX_FEATURES)
tokenizer.fit_on_texts(X_train['clean'])
sequences_train = tokenizer.texts_to_sequences(X_train['clean'])
sequences_test = tokenizer.texts_to_sequences(X_test['clean'])

X_TRAIN = sequence.pad_sequences(sequences_train, maxlen = MAXLEN,padding='post')
X_TEST = sequence.pad_sequences(sequences_test, maxlen = MAXLEN,padding='post')


Y_TRAIN = Y_train #np_utils.to_categorical(Y_train, nb_classes)
Y_TEST = Y_test #np_utils.to_categorical(Y_test, nb_classes)

df.describe()

fig , axes = plt.subplots(2,3,figsize = (10,10), constrained_layout = True)
sns.countplot(ax=axes[0,0],x='toxic',data=df )
sns.countplot(ax=axes[0,1],x='severe_toxic',data=df)
sns.countplot(ax=axes[0,2],x='obscene',data=df)
sns.countplot(ax = axes[1,0],x='threat',data=df)
sns.countplot(ax=axes[1,1],x='insult',data=df)
sns.countplot(ax=axes[1,2],x='identity_hate',data=df)
plt.suptitle('No Of Classes Of Each Category')
plt.show()

fig , axes = plt.subplots(2,3,figsize = (10,10), constrained_layout = True)
sns.countplot(ax=axes[0,0],x='toxic',data = df_test_label )
sns.countplot(ax=axes[0,1],x='severe_toxic',data = df_test_label)
sns.countplot(ax=axes[0,2],x='obscene',data = df_test_label)
sns.countplot(ax = axes[1,0],x='threat',data = df_test_label)
sns.countplot(ax=axes[1,1],x='insult',data = df_test_label)
sns.countplot(ax=axes[1,2],x='identity_hate',data = df_test_label)
plt.suptitle('No Of Classes Of Each Category')
plt.show()

from gensim.scripts.glove2word2vec import glove2word2vec
glove_input_file = 'glove.6B.300d.txt'
word2vec_output_file = 'glove.6B.300d.txt.w2v'
glove2word2vec(glove_input_file, word2vec_output_file)

from gensim.models import KeyedVectors
# load the Stanford GloVe model 
model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)
# model.save("Glove_word2vec.model")

# model['computer'] 
words, vectors = [], []
for k,v in model.vocab.items():
    words.append(k)
    vectors.append(model[k])
# model.vocab.keys()

import numpy
vocab_size = len(tokenizer.word_index) + 1
embedding_matrix = numpy.zeros((vocab_size, 300))
for word, index in tokenizer.word_index.items():
    if word in model:
        embedding_matrix[index] = model[word]

batch_size = 128
from  keras import Sequential
from keras.layers import *
lstm_model = Sequential()
lstm_model.add(Embedding(vocab_size, 300, weights = [embedding_matrix], trainable = False))
lstm_model.add(LSTM(128))
lstm_model.add(Dense(6, activation='sigmoid'))
lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])

print('Training...')
history = lstm_model.fit(X_TRAIN, Y_TRAIN, batch_size=batch_size, epochs = 7, validation_split=0.2)

score, acc = lstm_model.evaluate(X_TEST, Y_TEST,
                            batch_size=batch_size)

print('Test accuracy:', acc)
print("Generating test predictions...")

print(lstm_model.summary())

from keras.utils import plot_model
plot_model(lstm_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

score = lstm_model.evaluate(X_TEST, Y_TEST, verbose=1)

print("Loss:", score[0])
print("Test Accuracy:", score[1])

import matplotlib.pyplot as plt

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

def predict_Class(df,threshode = 0.5, ):
  df[df >= threshode] = 1
  df[df < threshode] = 0

from sklearn.metrics import classification_report
train_predict = lstm_model.predict(X_TRAIN, verbose=0)
test_predict = lstm_model.predict(X_TEST, verbose=0)
predict_Class(train_predict)
predict_Class(test_predict)

print(">> training set \n")
from sklearn.metrics import classification_report
print(classification_report(Y_TRAIN, train_predict,target_names= ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',
       'identity_hate']))

print(">> testing set \n")
print(classification_report(Y_TEST, test_predict,target_names= ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',
       'identity_hate']))

input_1 = Input(shape=(MAXLEN,))
embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], trainable=False)(input_1)
LSTM_Layer1 = LSTM(128)(embedding_layer)

output1 = Dense(1, activation='sigmoid')(LSTM_Layer1)
output2 = Dense(1, activation='sigmoid')(LSTM_Layer1)
output3 = Dense(1, activation='sigmoid')(LSTM_Layer1)
output4 = Dense(1, activation='sigmoid')(LSTM_Layer1)
output5 = Dense(1, activation='sigmoid')(LSTM_Layer1)
output6 = Dense(1, activation='sigmoid')(LSTM_Layer1)
from keras.models import Model
model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])

print(model.summary())

from keras.utils import plot_model
plot_model(model, to_file='model_plot_2.png', show_shapes = True, show_layer_names = True)

y1_train = Y_TRAIN[:,0]
y1_test =  Y_TEST[:,0]

y2_train = Y_TRAIN[:,1]
y2_test =  Y_TEST[:,1]

y3_train = Y_TRAIN[:,2]
y3_test =  Y_TEST[:,2]

y4_train = Y_TRAIN[:,3]
y4_test =  Y_TEST[:,3]

y5_train = Y_TRAIN[:,4]
y5_test =  Y_TEST[:,4]

y6_train = Y_TRAIN[:,5]
y6_test =  Y_TEST[:,5]
history1 = model.fit(x= X_TRAIN, y=[y1_train, y2_train, y3_train, y4_train, y5_train, y6_train], batch_size= batch_size , epochs=5, verbose=1, validation_split=0.2)

score = model.evaluate(x=X_TEST, y=[y1_test, y2_test, y3_test, y4_test, y5_test, y6_test], verbose=1)

print("Test Score:", score[0])
print("Test Accuracy:", score[1])

import matplotlib.pyplot as plt
plt.plot(history1.history['dense_12_acc'])
plt.plot(history1.history['val_dense_12_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()