# -*- coding: utf-8 -*-
"""multi-label_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Up1LxTPMOHmsjflpXbW-m3TbAeoyd_pl
"""

# #The dataset we use can be downloaded from Kaggle(https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview)
!unzip train.csv.zip
import pandas as pd
df = pd.read_csv("train.csv")

import pandas as pd
!unzip test.csv.zip
df_test = pd.read_csv("test.csv")

!unzip test_labels.csv.zip
df_test_label = pd.read_csv("test_labels.csv")

!pip install transformers

from transformers import BertTokenizer
import torch
MAX_LENGTH  = 30
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
def preprocessing_for_bert(data):
    input_ids = []
    attention_masks = []

    # For every sentence...
    for sent in data:
        encoded_sent = tokenizer.encode_plus(
            text = sent,  
            add_special_tokens = True,        # Add `[CLS]` and `[SEP]`
            max_length = MAX_LENGTH,                  
            pad_to_max_length = True,        
            #return_tensors='pt',           # Return PyTorch tensor
            truncation = True,
            return_attention_mask = True      # Return attention mask
            )
        input_ids.append(encoded_sent.get('input_ids'))
        attention_masks.append(encoded_sent.get('attention_mask'))

    # Convert lists to tensors
    input_ids = torch.tensor(input_ids)
    attention_masks = torch.tensor(attention_masks)

    return input_ids, attention_masks

labels = df[["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]]
label_counts = labels.sum(axis=0)

from sklearn.model_selection import train_test_split

def split_train_test(X, Y, test_size = 0.2, shuffle_state = True ):
    FEATURES = ['comment_text']
    X_train, X_test, Y_train, Y_test =\
                                        train_test_split(X[FEATURES],Y,  test_size=0.1, random_state=2021)

    
    X_train = X_train.comment_text.to_list()
    X_test = X_test.comment_text.to_list()
    return X_train, X_test, Y_train, Y_test 
      
X_train, X_val, Y_train, Y_val = split_train_test(df, labels.values)

train_inputs, train_masks = preprocessing_for_bert(X_train)
val_inputs, val_masks = preprocessing_for_bert(X_val)

train_labels = torch.tensor(Y_train)
val_labels = torch.tensor(Y_val)

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.
batch_size = 64

# Create the DataLoader for our training set
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

# Create the DataLoader for our validation set
val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

!pip install pytorch-transformers

!pip install pysnooper -q
import pysnooper
import torch
import torch.nn as nn
from transformers import BertModel

class MultiLabel(nn.Module):
    @pysnooper.snoop()
    def __init__(self, freeze_bert = False):
        super(MultiLabel, self).__init__()
        self.num_labels = 6
        D_in = 768 #config.hidden_size (of bert)
        H = 50  #hidden size of our model 
        
        DROPOUT = 0.1 #config.hidden_dropout_prob
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = nn.Dropout(DROPOUT) 
        self.classifier = torch.nn.Linear(D_in, self.num_labels)
        
        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False
    
               
    def forward(self, tokens_tensors, masks_tensors):
           
        outputs = self.bert(input_ids = tokens_tensors,
                            attention_mask = masks_tensors)
        logits = self.classifier(outputs[0][:, 0, :])
        return logits

# model = MultiLabel()

from transformers import AdamW, get_linear_schedule_with_warmup
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def initialize_model(epochs=4):
    
    bert_classifier = MultiLabel()
    bert_classifier.to(device)
    optimizer = AdamW(bert_classifier.parameters(),
                      lr=5e-5,   
                      eps=1e-8   
                      )
    total_steps = len(train_dataloader) * epochs

    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps = 0,
                                                num_training_steps = total_steps)
    return bert_classifier, optimizer, scheduler

import random
import time

loss_fct = nn.BCEWithLogitsLoss()

def set_seed(seed_value = 62):

    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    torch.cuda.manual_seed_all(seed_value)

NUM_LABELS = 6

from tqdm import tqdm 

print("start training...")
def train(model, train_dataloader, val_dataloader = None, epochs = 4, evaluation = False):
    for epoch_i in tqdm(range(epochs)):
        
        print(f"\n{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} |{'AUC_ROC':^9} | {'Elapsed':^9}")
        print("-"*70)

        t0_epoch, t0_batch = time.time(), time.time()
        total_loss, batch_loss, batch_counts = 0, 0, 0
        model.train()

        for step, batch in enumerate(train_dataloader):
            batch_counts += 1
            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

            model.zero_grad()
            output = model(b_input_ids, b_attn_mask)
            
            loss = loss_fct(output.view(-1, 6), b_labels.view(-1, 6).type_as(output))
            batch_loss += loss.item()
            total_loss += loss.item()

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #
            optimizer.step()
            scheduler.step()

            # Print the loss values and time elapsed for every 20 batches
            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):
                # Calculate time elapsed for 20 batches
                time_elapsed = time.time() - t0_batch

                # Print training results
                print(f"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9}| {'-':^9} | {time_elapsed:^9.2f}")

                batch_loss, batch_counts = 0, 0
                t0_batch = time.time()
        avg_train_loss = total_loss / len(train_dataloader)
    
        print("Evaluation...")
        if evaluation == True:
            
            val_loss, val_accuracy, val_aucroc = evaluate(model, val_dataloader)

            time_elapsed = time.time() - t0_epoch
            print(f"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {val_aucroc:^9.2f} | {time_elapsed:^9.2f}")
            print("-"*70)
        print("\n")
    
    print("Training complete!")

import numpy as np

def evaluate(model, val_dataloader,thresh = 0.5, sigmoid = True ):
    """After the completion of each training epoch, measure the model's performance
    on our validation set.
    """
    model.eval()

    # Tracking variables
    val_accuracy = []
    val_loss = []
    val_aucroc = []
    # For each batch in our validation set...
    for batch in val_dataloader:
        
        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)

        # Compute loss    
        loss = loss_fct(logits, b_labels.type_as(logits))
        val_loss.append(loss.item())

        # Calculate the accuracy rate
        if sigmoid: logits = logits.sigmoid()
    
        #===========

        y_preds = (logits > 0.5).float().cpu().numpy()
        y_true = b_labels.byte().float().cpu().numpy()

        from sklearn.metrics import accuracy_score
        acc = accuracy_score(y_true, y_preds)
        
        from sklearn.metrics import roc_curve, auc

        # Compute ROC curve and ROC area for each class
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        logits = logits.float().cpu().numpy()
        for i in range(6):
            fpr[i], tpr[i], _ = roc_curve(y_true[:, i], logits[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        
        # Compute micro-average ROC curve and ROC area
        fpr["micro_avg"], tpr["micro_avg"], _ = roc_curve(y_true.ravel(), logits.ravel())
        roc_auc["micro_avg"] = auc(fpr["micro_avg"], tpr["micro_avg"])
        val_aucroc.append(roc_auc["micro_avg"])
        
        from sklearn.metrics import classification_report
        report = classification_report(y_true, y_preds, output_dict=True)
        print(report)
        # macro_precision =  report['macro avg']['precision'] 
        # macro_recall = report['macro avg']['recall']    
        # macro_f1 = report['macro avg']['f1-score']
        # f1 = report['micro avg']['f1-score']
        #====
        val_accuracy.append(acc)
        
    # # Compute the average accuracy and loss over the validation set.
   
    val_loss = np.mean(val_loss)
    val_accuracy = np.mean(val_accuracy)
    val_auc_roc = np.mean(val_aucroc)
    
    
    return val_loss, val_accuracy,val_auc_roc

bert_classifier, optimizer, scheduler = initialize_model(epochs=5)

al_loss, val_accuracy, val_aucroc = evaluate(bert_classifier, val_dataloader)

# val_loss, val_accuracy, val_aucroc = evaluate(bert_classifier, val_dataloader)
print(val_loss, val_accuracy, val_aucroc)

# print(bert_classifier)

train(bert_classifier, train_dataloader, val_dataloader, epochs=5, evaluation=True)

torch.save(bert_classifier.state_dict(), "here")

